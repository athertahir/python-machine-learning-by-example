{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "Source codes for Python Machine Learning By Example 2nd Edition (Packt Publishing)\n", 
        "Chapter 4: Detecting Spam Email with Naive Bayes\n", 
        "Author: Yuxi (Hayden) Liu"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "# -*- coding: utf-8 -*-\n", 
        "\n", 
        "import glob\n", 
        "import os\n", 
        "import numpy as np\n", 
        "\n", 
        "\n", 
        "file_path = 'enron1/ham/0007.1999-12-14.farmer.ham.txt'\n", 
        "with open(file_path, 'r') as infile:\n", 
        "    ham_sample = infile.read()\n", 
        "\n", 
        "print(ham_sample)\n", 
        "\n", 
        "file_path = 'enron1/spam/0058.2003-12-21.GP.spam.txt'\n", 
        "with open(file_path, 'r') as infile:\n", 
        "    spam_sample = infile.read()\n", 
        "\n", 
        "print(spam_sample)\n", 
        "\n", 
        "\n", 
        "emails, labels = [], []\n", 
        "\n", 
        "file_path = 'enron1/spam/'\n", 
        "for filename in glob.glob(os.path.join(file_path, '*.txt')):\n", 
        "    with open(filename, 'r', encoding=\"ISO-8859-1\") as infile:\n", 
        "        emails.append(infile.read())\n", 
        "        labels.append(1)\n", 
        "\n", 
        "file_path = 'enron1/ham/'\n", 
        "for filename in glob.glob(os.path.join(file_path, '*.txt')):\n", 
        "    with open(filename, 'r', encoding=\"ISO-8859-1\") as infile:\n", 
        "        emails.append(infile.read())\n", 
        "        labels.append(0)\n", 
        "\n", 
        "print(len(labels))\n", 
        "\n", 
        "print(len(emails))\n", 
        "\n", 
        "\n", 
        "\n", 
        "\n", 
        "def is_letter_only(word):\n", 
        "    return word.isalpha()\n", 
        "\n", 
        "from nltk.corpus import names\n", 
        "all_names = set(names.words())\n", 
        "\n", 
        "from nltk.stem import WordNetLemmatizer\n", 
        "lemmatizer = WordNetLemmatizer()\n", 
        "\n", 
        "def clean_text(docs):\n", 
        "    docs_cleaned = []\n", 
        "    for doc in docs:\n", 
        "        doc = doc.lower()\n", 
        "        doc_cleaned = ' '.join(lemmatizer.lemmatize(word) for word in doc.split() if is_letter_only(word) and word not in all_names)\n", 
        "        docs_cleaned.append(doc_cleaned)\n", 
        "    return docs_cleaned\n", 
        "\n", 
        "emails_cleaned = clean_text(emails)\n", 
        "\n", 
        "from sklearn.feature_extraction.text import CountVectorizer\n", 
        "cv = CountVectorizer(stop_words=\"english\", max_features=1000, max_df=0.5, min_df=2)\n", 
        "\n", 
        "docs_cv = cv.fit_transform(emails_cleaned)\n", 
        "print(docs_cv[0])\n", 
        "\n", 
        "terms = cv.get_feature_names()\n", 
        "print(terms[932])\n", 
        "print(terms[968])\n", 
        "print(terms[715])\n", 
        "\n", 
        "\n", 
        "\n", 
        "def get_label_index(labels):\n", 
        "    from collections import defaultdict\n", 
        "    label_index = defaultdict(list)\n", 
        "    for index, label in enumerate(labels):\n", 
        "        label_index[label].append(index)\n", 
        "    return label_index\n", 
        "\n", 
        "\n", 
        "def get_prior(label_index):\n", 
        "    \"\"\"\n", 
        "    Compute prior based on training samples\n", 
        "    @param label_index: grouped sample indices by class\n", 
        "    @return: dictionary, with class label as key, corresponding prior as the value\n", 
        "    \"\"\"\n", 
        "    prior = {label: len(index) for label, index in label_index.items()}\n", 
        "    total_count = sum(prior.values())\n", 
        "    for label in prior:\n", 
        "        prior[label] /= float(total_count)\n", 
        "    return prior\n", 
        "\n", 
        "\n", 
        "label_index = get_label_index(labels)\n", 
        "prior = get_prior(label_index)\n", 
        "print('Prior:', prior)\n", 
        "\n", 
        "\n", 
        "def get_likelihood(term_matrix, label_index, smoothing=0):\n", 
        "    \"\"\"\n", 
        "    Compute likelihood based on training samples\n", 
        "    @param term_matrix: sparse matrix of the term frequency features\n", 
        "    @param label_index: grouped sample indices by class\n", 
        "    @param smoothing: integer, additive Laplace smoothing parameter\n", 
        "    @return: dictionary, with class as key, corresponding conditional probability P(feature|class) vector as value\n", 
        "    \"\"\"\n", 
        "    likelihood = {}\n", 
        "    for label, index in label_index.items():\n", 
        "        likelihood[label] = term_matrix[index, :].sum(axis=0) + smoothing\n", 
        "        likelihood[label] = np.asarray(likelihood[label])[0]\n", 
        "        total_count = likelihood[label].sum()\n", 
        "        likelihood[label] = likelihood[label] / float(total_count)\n", 
        "    return likelihood\n", 
        "\n", 
        "smoothing = 1\n", 
        "likelihood = get_likelihood(docs_cv, label_index, smoothing)\n", 
        "\n", 
        "print(len(likelihood[0]))\n", 
        "\n", 
        "print(likelihood[0][:5])\n", 
        "print(likelihood[1][:5])\n", 
        "\n", 
        "\n", 
        "\n", 
        "def get_posterior(term_matrix, prior, likelihood):\n", 
        "    \"\"\"\n", 
        "    Compute posterior of testing samples, based on prior and likelihood\n", 
        "    @param term_matrix: sparse matrix of the term frequency features\n", 
        "    @param prior: dictionary, with class label as key, corresponding prior as the value\n", 
        "    @param likelihood: dictionary, with class label as key, corresponding conditional probability vector as value\n", 
        "    @return: dictionary, with class label as key, corresponding posterior as value\n", 
        "    \"\"\"\n", 
        "    num_docs = term_matrix.shape[0]\n", 
        "    posteriors = []\n", 
        "    for i in range(num_docs):\n", 
        "        # posterior is proportional to prior * likelihood\n", 
        "        # = exp(log(prior * likelihood))\n", 
        "        # = exp(log(prior) + log(likelihood))\n", 
        "        posterior = {key: np.log(prior_label) for key, prior_label in prior.items()}\n", 
        "        for label, likelihood_label in likelihood.items():\n", 
        "            term_document_vector = term_matrix.getrow(i)\n", 
        "            counts = term_document_vector.data\n", 
        "            indices = term_document_vector.indices\n", 
        "            for count, index in zip(counts, indices):\n", 
        "                posterior[label] += np.log(likelihood_label[index]) * count\n", 
        "        # exp(-1000):exp(-999) will cause zero division error,\n", 
        "        # however it equates to exp(0):exp(1)\n", 
        "        min_log_posterior = min(posterior.values())\n", 
        "        for label in posterior:\n", 
        "            try:\n", 
        "                posterior[label] = np.exp(posterior[label] - min_log_posterior)\n", 
        "            except:\n", 
        "                posterior[label] = float('inf')\n", 
        "        # normalize so that all sums up to 1\n", 
        "        sum_posterior = sum(posterior.values())\n", 
        "        for label in posterior:\n", 
        "            if posterior[label] == float('inf'):\n", 
        "                posterior[label] = 1.0\n", 
        "            else:\n", 
        "                posterior[label] /= sum_posterior\n", 
        "        posteriors.append(posterior.copy())\n", 
        "    return posteriors\n", 
        "\n", 
        "\n", 
        "\n", 
        "emails_test = [\n", 
        "    '''Subject: flat screens\n", 
        "    hello ,\n", 
        "    please call or contact regarding the other flat screens requested .\n", 
        "    trisha tlapek - eb 3132 b\n", 
        "    michael sergeev - eb 3132 a\n", 
        "    also the sun blocker that was taken away from eb 3131 a .\n", 
        "    trisha should two monitors also michael .\n", 
        "    thanks\n", 
        "    kevin moore''',\n", 
        "    '''Subject: let ' s stop the mlm insanity !\n", 
        "    still believe you can earn $ 100 , 000 fast in mlm ? get real !\n", 
        "    get emm , a brand new system that replaces mlm with something that works !\n", 
        "    start earning 1 , 000 ' s now ! up to $ 10 , 000 per week doing simple online tasks .\n", 
        "    free info - breakfree @ luxmail . com - type \" send emm info \" in the subject box .\n", 
        "    this message is sent in compliance of the proposed bill section 301 . per section 301 , paragraph ( a ) ( 2 ) ( c ) of s . 1618 . further transmission to you by the sender of this e - mail may be stopped at no cost to you by sending a reply to : \" email address \" with the word remove in the subject line .\n", 
        "    ''',\n", 
        "]\n", 
        "\n", 
        "emails_cleaned_test = clean_text(emails_test)\n", 
        "term_docs_test = cv.transform(emails_cleaned_test)\n", 
        "\n", 
        "\n", 
        "posterior = get_posterior(term_docs_test, prior, likelihood)\n", 
        "print(posterior)\n", 
        "\n", 
        "\n", 
        "\n", 
        "from sklearn.model_selection import train_test_split\n", 
        "X_train, X_test, Y_train, Y_test = train_test_split(emails_cleaned, labels, test_size=0.33, random_state=42)\n", 
        "\n", 
        "print(len(X_train), len(Y_train))\n", 
        "len(X_test), len(Y_test)\n", 
        "\n", 
        "term_docs_train = cv.fit_transform(X_train)\n", 
        "\n", 
        "label_index = get_label_index(Y_train)\n", 
        "prior = get_prior(label_index)\n", 
        "likelihood = get_likelihood(term_docs_train, label_index, smoothing)\n", 
        "\n", 
        "term_docs_test = cv.transform(X_test)\n", 
        "\n", 
        "\n", 
        "posterior = get_posterior(term_docs_test, prior, likelihood)\n", 
        "\n", 
        "correct = 0.0\n", 
        "for pred, actual in zip(posterior, Y_test):\n", 
        "    if actual == 1:\n", 
        "        if pred[1] >= 0.5:\n", 
        "            correct += 1\n", 
        "    elif pred[0] > 0.5:\n", 
        "        correct += 1\n", 
        "\n", 
        "print('The accuracy on {0} testing samples is: {1:.1f}%'.format(len(Y_test), correct/len(Y_test)*100))\n", 
        "\n", 
        "\n", 
        "\n", 
        "\n", 
        "from sklearn.naive_bayes import MultinomialNB\n", 
        "clf = MultinomialNB(alpha=1.0, fit_prior=True)\n", 
        "clf.fit(term_docs_train, Y_train)\n", 
        "prediction_prob = clf.predict_proba(term_docs_test)\n", 
        "\n", 
        "print(prediction_prob[0:10])\n", 
        "\n", 
        "prediction = clf.predict(term_docs_test)\n", 
        "\n", 
        "print(prediction[:10])\n", 
        "\n", 
        "accuracy = clf.score(term_docs_test, Y_test)\n", 
        "\n", 
        "print('The accuracy using MultinomialNB is: {0:.1f}%'.format(accuracy*100))\n", 
        "\n", 
        "\n", 
        "\n", 
        "\n", 
        "from sklearn.metrics import confusion_matrix\n", 
        "print(confusion_matrix(Y_test, prediction, labels=[0, 1]))\n", 
        "\n", 
        "from sklearn.metrics import precision_score, recall_score, f1_score\n", 
        "precision_score(Y_test, prediction, pos_label=1)\n", 
        "recall_score(Y_test, prediction, pos_label=1)\n", 
        "f1_score(Y_test, prediction, pos_label=1)\n", 
        "\n", 
        "f1_score(Y_test, prediction, pos_label=0)\n", 
        "\n", 
        "from sklearn.metrics import classification_report\n", 
        "report = classification_report(Y_test, prediction)\n", 
        "print(report)\n", 
        "\n", 
        "\n", 
        "\n", 
        "\n", 
        "pos_prob = prediction_prob[:, 1]\n", 
        "thresholds = np.arange(0.0, 1.2, 0.1)\n", 
        "true_pos, false_pos = [0]*len(thresholds), [0]*len(thresholds)\n", 
        "for pred, y in zip(pos_prob, Y_test):\n", 
        "    for i, threshold in enumerate(thresholds):\n", 
        "        if pred >= threshold:\n", 
        "            if y == 1:\n", 
        "                true_pos[i] += 1\n", 
        "            else:\n", 
        "                false_pos[i] += 1\n", 
        "        else:\n", 
        "            break\n", 
        "\n", 
        "true_pos_rate = [tp / 516.0 for tp in true_pos]\n", 
        "false_pos_rate = [fp / 1191.0 for fp in false_pos]\n", 
        "\n", 
        "\n", 
        "%matplotlib inline\n", 
        "import matplotlib.pyplot as plt\n", 
        "plt.figure()\n", 
        "lw = 2\n", 
        "plt.plot(false_pos_rate, true_pos_rate, color='darkorange',\n", 
        "         lw=lw)\n", 
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n", 
        "plt.xlim([0.0, 1.0])\n", 
        "plt.ylim([0.0, 1.05])\n", 
        "plt.xlabel('False Positive Rate')\n", 
        "plt.ylabel('True Positive Rate')\n", 
        "plt.title('Receiver Operating Characteristic')\n", 
        "plt.legend(loc=\"lower right\")\n", 
        "plt.show()\n", 
        "\n", 
        "\n", 
        "\n", 
        "\n", 
        "from sklearn.metrics import roc_auc_score\n", 
        "roc_auc_score(Y_test, pos_prob)\n", 
        "\n", 
        "\n", 
        "\n", 
        "from sklearn.model_selection import StratifiedKFold\n", 
        "k = 10\n", 
        "k_fold = StratifiedKFold(n_splits=k, random_state=42)\n", 
        "cleaned_emails_np = np.array(emails_cleaned)\n", 
        "labels_np = np.array(labels)\n", 
        "\n", 
        "max_features_option = [2000, 8000, None]\n", 
        "smoothing_factor_option = [0.5, 1.0, 2.0, 4.0]\n", 
        "fit_prior_option = [True, False]\n", 
        "\n", 
        "max_features_option = [None]\n", 
        "smoothing_factor_option = [4.0, 10, 16, 20, 32]\n", 
        "fit_prior_option = [True, False]\n", 
        "\n", 
        "\n", 
        "auc_record = {}\n", 
        "\n", 
        "for train_indices, test_indices in k_fold.split(emails_cleaned, labels):\n", 
        "    X_train, X_test = cleaned_emails_np[train_indices], cleaned_emails_np[test_indices]\n", 
        "    Y_train, Y_test = labels_np[train_indices], labels_np[test_indices]\n", 
        "    for max_features in max_features_option:\n", 
        "        if max_features not in auc_record:\n", 
        "            auc_record[max_features] = {}\n", 
        "        cv = CountVectorizer(stop_words=\"english\", max_features=max_features, max_df=0.5, min_df=2)\n", 
        "        term_docs_train = cv.fit_transform(X_train)\n", 
        "        term_docs_test = cv.transform(X_test)\n", 
        "        for alpha in smoothing_factor_option:\n", 
        "            if alpha not in auc_record[max_features]:\n", 
        "                auc_record[max_features][alpha] = {}\n", 
        "            for fit_prior in fit_prior_option:\n", 
        "                clf = MultinomialNB(alpha=alpha, fit_prior=fit_prior)\n", 
        "                clf.fit(term_docs_train, Y_train)\n", 
        "                prediction_prob = clf.predict_proba(term_docs_test)\n", 
        "                pos_prob = prediction_prob[:, 1]\n", 
        "                auc = roc_auc_score(Y_test, pos_prob)\n", 
        "                auc_record[max_features][alpha][fit_prior] = auc + auc_record[max_features][alpha].get(fit_prior, 0.0)\n", 
        "\n", 
        "\n", 
        "\n", 
        "print('max features  smoothing  fit prior  auc')\n", 
        "for max_features, max_feature_record in auc_record.items():\n", 
        "    for smoothing, smoothing_record in max_feature_record.items():\n", 
        "        for fit_prior, auc in smoothing_record.items():\n", 
        "            print('       {0}      {1}      {2}    {3:.5f}'.format(max_features, smoothing, fit_prior, auc/k))\n", 
        "\n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}
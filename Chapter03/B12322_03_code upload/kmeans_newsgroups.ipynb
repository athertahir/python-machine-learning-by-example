{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "Source codes for Python Machine Learning By Example 2nd Edition (Packt Publishing)\n", 
        "Chapter 3: Mining the 20 Newsgroups Dataset with Clustering and Topic Modeling Algorithms\n", 
        "Author: Yuxi (Hayden) Liu"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n", 
        "\n", 
        "categories = [\n", 
        "    'alt.atheism',\n", 
        "    'talk.religion.misc',\n", 
        "    'comp.graphics',\n", 
        "    'sci.space',\n", 
        "]\n", 
        "\n", 
        "\n", 
        "groups = fetch_20newsgroups(subset='all', categories=categories)\n", 
        "\n", 
        "\n", 
        "labels = groups.target\n", 
        "label_names = groups.target_names\n", 
        "\n", 
        "\n", 
        "\n", 
        "def is_letter_only(word):\n", 
        "    for char in word:\n", 
        "        if not char.isalpha():\n", 
        "            return False\n", 
        "    return True\n", 
        "\n", 
        "\n", 
        "\n", 
        "from nltk.corpus import names\n", 
        "all_names = set(names.words())\n", 
        "\n", 
        "\n", 
        "\n", 
        "\n", 
        "from nltk.stem import WordNetLemmatizer\n", 
        "lemmatizer = WordNetLemmatizer()\n", 
        "\n", 
        "data_cleaned = []\n", 
        "\n", 
        "for doc in groups.data:\n", 
        "    doc = doc.lower()\n", 
        "    doc_cleaned = ' '.join(lemmatizer.lemmatize(word) for word in doc.split() if is_letter_only(word) and word not in all_names)\n", 
        "    data_cleaned.append(doc_cleaned)\n", 
        "\n", 
        "\n", 
        "from sklearn.feature_extraction.text import CountVectorizer\n", 
        "count_vector = CountVectorizer(stop_words=\"english\", max_features=None, max_df=0.5, min_df=2)\n", 
        "\n", 
        "from sklearn.feature_extraction.text import TfidfVectorizer\n", 
        "tfidf_vector = TfidfVectorizer(stop_words='english', max_features=None, max_df=0.5, min_df=2)\n", 
        "\n", 
        "data = tfidf_vector.fit_transform(data_cleaned)\n", 
        "\n", 
        "\n", 
        "from sklearn.cluster import KMeans\n", 
        "\n", 
        "k = 4\n", 
        "kmeans = KMeans(n_clusters=k, random_state=42)\n", 
        "\n", 
        "kmeans.fit(data)\n", 
        "\n", 
        "clusters = kmeans.labels_\n", 
        "\n", 
        "\n", 
        "\n", 
        "from collections import Counter\n", 
        "print(Counter(clusters))\n", 
        "\n", 
        "import numpy as np\n", 
        "cluster_label = {i: labels[np.where(clusters == i)] for i in range(k)}\n", 
        "\n", 
        "terms = tfidf_vector.get_feature_names()\n", 
        "centroids = kmeans.cluster_centers_\n", 
        "for cluster, index_list in cluster_label.items():\n", 
        "    counter = Counter(cluster_label[cluster])\n", 
        "    print('cluster_{}: {} samples'.format(cluster, len(index_list)))\n", 
        "    for label_index, count in sorted(counter.items(), key=lambda x: x[1], reverse=True):\n", 
        "        print('{}: {} samples'.format(label_names[label_index], count))\n", 
        "    print('Top 10 terms:')\n", 
        "    for ind in centroids[cluster].argsort()[-10:]:\n", 
        "        print(' %s' % terms[ind], end=\"\")\n", 
        "    print()\n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}
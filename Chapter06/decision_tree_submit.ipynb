{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "Source codes for Python Machine Learning By Example 2nd Edition (Packt Publishing)\n", 
        "Chapter 6: Predicting Online Ads Click-through with Tree-Based Algorithms\n", 
        "Author: Yuxi (Hayden) Liu"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "import matplotlib.pyplot as plt\n", 
        "import numpy as np\n", 
        "\n", 
        "\n", 
        "# Plot Gini Impurity in binary case\n", 
        "pos_fraction = np.linspace(0.00, 1.00, 1000)\n", 
        "gini = 1 - pos_fraction**2 - (1-pos_fraction)**2\n", 
        "plt.plot(pos_fraction, gini)\n", 
        "plt.xlabel('Positive fraction')\n", 
        "plt.ylabel('Gini Impurity')\n", 
        "plt.ylim(0, 1)\n", 
        "plt.show()\n", 
        "\n", 
        "\n", 
        "# Given labels of a data set, the Gini Impurity calculation function\n", 
        "def gini_impurity(labels):\n", 
        "    # When the set is empty, it is also pure\n", 
        "    if not labels:\n", 
        "        return 0\n", 
        "    # Count the occurrences of each label\n", 
        "    counts = np.unique(labels, return_counts=True)[1]\n", 
        "    fractions = counts / float(len(labels))\n", 
        "    return 1 - np.sum(fractions ** 2)\n", 
        "\n", 
        "print('{0:.4f}'.format(gini_impurity([1, 1, 0, 1, 0])))\n", 
        "print('{0:.4f}'.format(gini_impurity([1, 1, 0, 1, 0, 0])))\n", 
        "print('{0:.4f}'.format(gini_impurity([1, 1, 1, 1])))\n", 
        "\n", 
        "\n", 
        "# Plot entropy in binary case\n", 
        "pos_fraction = np.linspace(0.00, 1.00, 1000)\n", 
        "ent = - (pos_fraction * np.log2(pos_fraction) + (1 - pos_fraction) * np.log2(1 - pos_fraction))\n", 
        "plt.plot(pos_fraction, ent)\n", 
        "plt.xlabel('Positive fraction')\n", 
        "plt.ylabel('Entropy')\n", 
        "plt.ylim(0, 1)\n", 
        "plt.show()\n", 
        "\n", 
        "\n", 
        "# Given labels of a data set, the entropy calculation function\n", 
        "def entropy(labels):\n", 
        "    if not labels:\n", 
        "        return 0\n", 
        "    counts = np.unique(labels, return_counts=True)[1]\n", 
        "    fractions = counts / float(len(labels))\n", 
        "    return - np.sum(fractions * np.log2(fractions))\n", 
        "\n", 
        "print('{0:.4f}'.format(entropy([1, 1, 0, 1, 0])))\n", 
        "print('{0:.4f}'.format(entropy([1, 1, 0, 1, 0, 0])))\n", 
        "print('{0:.4f}'.format(entropy([1, 1, 1, 1])))\n", 
        "\n", 
        "\n", 
        "def information_gain(y, mask, func=entropy):\n", 
        "    s1 = np.sum(mask)\n", 
        "    s2 = mask.size - s1\n", 
        "    if (s1 == 0 | s2 == 0): return 0\n", 
        "    return func(y) - s1 / float(s1 + s2) * func(y[mask]) - s2 / float(s1 + s2) * func(y[np.logical_not(mask)])\n", 
        "\n", 
        "\n", 
        "criterion_function = {'gini': gini_impurity, 'entropy': entropy}\n", 
        "def weighted_impurity(groups, criterion='gini'):\n", 
        "    \"\"\"\n", 
        "    Calculate weighted impurity of children after a split\n", 
        "    @param groups: list of children, and a child consists a list of class labels\n", 
        "    @param criterion: metric to measure the quality of a split, 'gini' for Gini Impurity or 'entropy' for Information Gain\n", 
        "    @return: float, weighted impurity\n", 
        "    \"\"\"\n", 
        "    total = sum(len(group) for group in groups)\n", 
        "    weighted_sum = 0.0\n", 
        "    for group in groups:\n", 
        "        weighted_sum += len(group) / float(total) * criterion_function[criterion](group)\n", 
        "    return weighted_sum\n", 
        "\n", 
        "\n", 
        "children_1 = [[1, 0, 1], [0, 1]]\n", 
        "children_2 = [[1, 1], [0, 0, 1]]\n", 
        "print('Entropy of #1 split: {0:.4f}'.format(weighted_impurity(children_1, 'entropy')))\n", 
        "print('Entropy of #2 split: {0:.4f}'.format(weighted_impurity(children_2, 'entropy')))\n", 
        "\n", 
        "\n", 
        "\n", 
        "def gini_impurity_np(labels):\n", 
        "    # When the set is empty, it is also pure\n", 
        "    if labels.size == 0:\n", 
        "        return 0\n", 
        "    # Count the occurrences of each label\n", 
        "    counts = np.unique(labels, return_counts=True)[1]\n", 
        "    fractions = counts / float(len(labels))\n", 
        "    return 1 - np.sum(fractions ** 2)\n", 
        "\n", 
        "\n", 
        "def entropy_np(labels):\n", 
        "    # When the set is empty, it is also pure\n", 
        "    if labels.size == 0:\n", 
        "        return 0\n", 
        "    counts = np.unique(labels, return_counts=True)[1]\n", 
        "    fractions = counts / float(len(labels))\n", 
        "    return - np.sum(fractions * np.log2(fractions))\n", 
        "\n", 
        "\n", 
        "criterion_function_np = {'gini': gini_impurity_np, 'entropy': entropy_np}\n", 
        "def weighted_impurity(groups, criterion='gini'):\n", 
        "    \"\"\"\n", 
        "    Calculate weighted impurity of children after a split\n", 
        "    @param groups: list of children, and a child consists a list of class labels\n", 
        "    @param criterion: metric to measure the quality of a split, 'gini' for Gini Impurity or 'entropy' for Information Gain\n", 
        "    @return: float, weighted impurity\n", 
        "    \"\"\"\n", 
        "    total = sum(len(group) for group in groups)\n", 
        "    weighted_sum = 0.0\n", 
        "    for group in groups:\n", 
        "        weighted_sum += len(group) / float(total) * criterion_function_np[criterion](group)\n", 
        "    return weighted_sum\n", 
        "\n", 
        "\n", 
        "def split_node(X, y, index, value):\n", 
        "    \"\"\"\n", 
        "    Split data set X, y based on a feature and a value\n", 
        "    @param X: numpy.ndarray, dataset feature\n", 
        "    @param y: numpy.ndarray, dataset target\n", 
        "    @param index: int, index of the feature used for splitting\n", 
        "    @param value: value of the feature used for splitting\n", 
        "    @return: list, list: left and right child, a child is in the format of [X, y]\n", 
        "    \"\"\"\n", 
        "    x_index = X[:, index]\n", 
        "    # if this feature is numerical\n", 
        "    if X[0, index].dtype.kind in ['i', 'f']:\n", 
        "        mask = x_index >= value\n", 
        "    # if this feature is categorical\n", 
        "    else:\n", 
        "        mask = x_index == value\n", 
        "    # split into left and right child\n", 
        "    left = [X[~mask, :], y[~mask]]\n", 
        "    right = [X[mask, :], y[mask]]\n", 
        "    return left, right\n", 
        "\n", 
        "\n", 
        "def get_best_split(X, y, criterion):\n", 
        "    \"\"\"\n", 
        "    Obtain the best splitting point and resulting children for the data set X, y\n", 
        "    @param X: numpy.ndarray, dataset feature\n", 
        "    @param y: numpy.ndarray, dataset target\n", 
        "    @param criterion: gini or entropy\n", 
        "    @return: dict {index: index of the feature, value: feature value, children: left and right children}\n", 
        "    \"\"\"\n", 
        "    best_index, best_value, best_score, children = None, None, 1, None\n", 
        "    for index in range(len(X[0])):\n", 
        "        for value in np.sort(np.unique(X[:, index])):\n", 
        "            groups = split_node(X, y, index, value)\n", 
        "            impurity = weighted_impurity([groups[0][1], groups[1][1]], criterion)\n", 
        "            if impurity < best_score:\n", 
        "                best_index, best_value, best_score, children = index, value, impurity, groups\n", 
        "    return {'index': best_index, 'value': best_value, 'children': children}\n", 
        "\n", 
        "\n", 
        "\n", 
        "def get_leaf(labels):\n", 
        "    # Obtain the leaf as the majority of the labels\n", 
        "    return np.bincount(labels).argmax()\n", 
        "\n", 
        "\n", 
        "\n", 
        "def split(node, max_depth, min_size, depth, criterion):\n", 
        "    \"\"\"\n", 
        "    Split children of a node to construct new nodes or assign them terminals\n", 
        "    @param node: dict, with children info\n", 
        "    @param max_depth: int, maximal depth of the tree\n", 
        "    @param min_size: int, minimal samples required to further split a child\n", 
        "    @param depth: int, current depth of the node\n", 
        "    @param criterion: gini or entropy\n", 
        "    \"\"\"\n", 
        "    left, right = node['children']\n", 
        "    del (node['children'])\n", 
        "    if left[1].size == 0:\n", 
        "        node['right'] = get_leaf(right[1])\n", 
        "        return\n", 
        "    if right[1].size == 0:\n", 
        "        node['left'] = get_leaf(left[1])\n", 
        "        return\n", 
        "    # Check if the current depth exceeds the maximal depth\n", 
        "    if depth >= max_depth:\n", 
        "        node['left'], node['right'] = get_leaf(left[1]), get_leaf(right[1])\n", 
        "        return\n", 
        "    # Check if the left child has enough samples\n", 
        "    if left[1].size <= min_size:\n", 
        "        node['left'] = get_leaf(left[1])\n", 
        "    else:\n", 
        "        # It has enough samples, we further split it\n", 
        "        result = get_best_split(left[0], left[1], criterion)\n", 
        "        result_left, result_right = result['children']\n", 
        "        if result_left[1].size == 0:\n", 
        "            node['left'] = get_leaf(result_right[1])\n", 
        "        elif result_right[1].size == 0:\n", 
        "            node['left'] = get_leaf(result_left[1])\n", 
        "        else:\n", 
        "            node['left'] = result\n", 
        "            split(node['left'], max_depth, min_size, depth + 1, criterion)\n", 
        "    # Check if the right child has enough samples\n", 
        "    if right[1].size <= min_size:\n", 
        "        node['right'] = get_leaf(right[1])\n", 
        "    else:\n", 
        "        # It has enough samples, we further split it\n", 
        "        result = get_best_split(right[0], right[1], criterion)\n", 
        "        result_left, result_right = result['children']\n", 
        "        if result_left[1].size == 0:\n", 
        "            node['right'] = get_leaf(result_right[1])\n", 
        "        elif result_right[1].size == 0:\n", 
        "            node['right'] = get_leaf(result_left[1])\n", 
        "        else:\n", 
        "            node['right'] = result\n", 
        "            split(node['right'], max_depth, min_size, depth + 1, criterion)\n", 
        "\n", 
        "\n", 
        "def train_tree(X_train, y_train, max_depth, min_size, criterion='gini'):\n", 
        "    \"\"\"\n", 
        "    Construction of a tree starts here\n", 
        "    @param X_train: list of training samples (feature)\n", 
        "    @param y_train: list of training samples (target)\n", 
        "    @param max_depth: int, maximal depth of the tree\n", 
        "    @param min_size: int, minimal samples required to further split a child\n", 
        "    @param criterion: gini or entropy\n", 
        "    \"\"\"\n", 
        "    X = np.array(X_train)\n", 
        "    y = np.array(y_train)\n", 
        "    root = get_best_split(X, y, criterion)\n", 
        "    split(root, max_depth, min_size, 1, criterion)\n", 
        "    return root\n", 
        "\n", 
        "\n", 
        "\n", 
        "CONDITION = {'numerical': {'yes': '>=', 'no': '<'},\n", 
        "             'categorical': {'yes': 'is', 'no': 'is not'}}\n", 
        "def visualize_tree(node, depth=0):\n", 
        "    if isinstance(node, dict):\n", 
        "        if node['value'].dtype.kind in ['i', 'f']:\n", 
        "            condition = CONDITION['numerical']\n", 
        "        else:\n", 
        "            condition = CONDITION['categorical']\n", 
        "        print('{}|- X{} {} {}'.format(depth * '  ', node['index'] + 1, condition['no'], node['value']))\n", 
        "        if 'left' in node:\n", 
        "            visualize_tree(node['left'], depth + 1)\n", 
        "        print('{}|- X{} {} {}'.format(depth * '  ', node['index'] + 1, condition['yes'], node['value']))\n", 
        "        if 'right' in node:\n", 
        "            visualize_tree(node['right'], depth + 1)\n", 
        "    else:\n", 
        "        print('{}[{}]'.format(depth * '  ', node))\n", 
        "\n", 
        "\n", 
        "X_train = [['tech', 'professional'],\n", 
        "           ['fashion', 'student'],\n", 
        "           ['fashion', 'professional'],\n", 
        "           ['sports', 'student'],\n", 
        "           ['tech', 'student'],\n", 
        "           ['tech', 'retired'],\n", 
        "           ['sports', 'professional']]\n", 
        "\n", 
        "y_train = [1,\n", 
        "           0,\n", 
        "           0,\n", 
        "           0,\n", 
        "           1,\n", 
        "           0,\n", 
        "           1]\n", 
        "\n", 
        "tree = train_tree(X_train, y_train, 2, 2)\n", 
        "visualize_tree(tree)\n", 
        "\n", 
        "\n", 
        "\n", 
        "\n", 
        "X_train_n = [[6, 7],\n", 
        "           [2, 4],\n", 
        "           [7, 2],\n", 
        "           [3, 6],\n", 
        "           [4, 7],\n", 
        "           [5, 2],\n", 
        "           [1, 6],\n", 
        "           [2, 0],\n", 
        "           [6, 3],\n", 
        "           [4, 1]]\n", 
        "\n", 
        "y_train_n = [0,\n", 
        "           0,\n", 
        "           0,\n", 
        "           0,\n", 
        "           0,\n", 
        "           1,\n", 
        "           1,\n", 
        "           1,\n", 
        "           1,\n", 
        "           1]\n", 
        "\n", 
        "tree = train_tree(X_train_n, y_train_n, 2, 2)\n", 
        "visualize_tree(tree)\n", 
        "\n", 
        "\n", 
        "from sklearn.tree import DecisionTreeClassifier\n", 
        "tree_sk = DecisionTreeClassifier(criterion='gini', max_depth=2, min_samples_split=2)\n", 
        "tree_sk.fit(X_train_n, y_train_n)\n", 
        "\n", 
        "from sklearn.tree import export_graphviz\n", 
        "export_graphviz(tree_sk, out_file='tree.dot', feature_names=['X1', 'X2'], impurity=False, filled=True, class_names=['0', '1'])\n", 
        "\n", 
        "\n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}
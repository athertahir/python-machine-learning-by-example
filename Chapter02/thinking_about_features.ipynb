{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "Source codes for Python Machine Learning By Example 2nd Edition (Packt Publishing)\n", 
        "Chapter 2: Exploring the 20 Newsgroups Dataset with Text Analysis Techniques\n", 
        "Author: Yuxi (Hayden) Liu"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n", 
        "\n", 
        "\n", 
        "groups = fetch_20newsgroups()\n", 
        "\n", 
        "\n", 
        "\n", 
        "from sklearn.feature_extraction.text import CountVectorizer\n", 
        "\n", 
        "count_vector = CountVectorizer(stop_words=\"english\",max_features=500)\n", 
        "data_count = count_vector.fit_transform(groups.data)\n", 
        "\n", 
        "print(count_vector.get_feature_names())\n", 
        "\n", 
        "data_count.toarray()[0]\n", 
        "\n", 
        "\n", 
        "\n", 
        "def is_letter_only(word):\n", 
        "    for char in word:\n", 
        "        if not char.isalpha():\n", 
        "            return False\n", 
        "    return True\n", 
        "\n", 
        "data_cleaned = []\n", 
        "for doc in groups.data:\n", 
        "    doc_cleaned = ' '.join(word for word in doc.split() if is_letter_only(word) )\n", 
        "    data_cleaned.append(doc_cleaned)\n", 
        "\n", 
        "\n", 
        "from sklearn.feature_extraction import stop_words\n", 
        "print(stop_words.ENGLISH_STOP_WORDS)\n", 
        "\n", 
        "\n", 
        "from nltk.corpus import names\n", 
        "all_names = set(names.words())\n", 
        "\n", 
        "\n", 
        "count_vector_sw = CountVectorizer(stop_words=\"english\", max_features=500)\n", 
        "\n", 
        "\n", 
        "from nltk.stem import WordNetLemmatizer\n", 
        "lemmatizer = WordNetLemmatizer()\n", 
        "\n", 
        "data_cleaned = []\n", 
        "\n", 
        "for doc in groups.data:\n", 
        "    doc = doc.lower()\n", 
        "    doc_cleaned = ' '.join(lemmatizer.lemmatize(word) for word in doc.split() if is_letter_only(word) and word not in all_names)\n", 
        "    data_cleaned.append(doc_cleaned)\n", 
        "\n", 
        "\n", 
        "data_cleaned_count = count_vector_sw.fit_transform(data_cleaned)\n", 
        "\n", 
        "print(count_vector_sw.get_feature_names())\n", 
        "\n", 
        "\n", 
        "\n", 
        "\n", 
        "\n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}